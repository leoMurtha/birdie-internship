# birdie-internship

# Parte Teórica
Pense numa aplicação onde esse script de crawler deve ser executado semanalmente para capturar novos produtos e atualizações nos dados dos produtos já capturados:

### Como você organizaria a arquitetura da aplicação/dados?
   Definiria uma arquitetura em camadas onde teria duas camadas a primeira sendo a camada responsável pela 
   extração dados, a segunda seria responsável pela base de dados sendo suas funções o processamento dos dados, 
   a inserção de novos produtos e atualizações nos produtos existentes a partir dos dados coletados pela primeira camada.
   Inline-style: 
                                            ![alt text](https://github.com/leoMurtha/birdie-internship/blob/master/data/arquitetura.png "Arquitetura Proposta")

  
### O que deixaria automatizado/agendado?
  
### O que monitorar pra acompanhar a saúde/qualidade da aplicação?
  
### Na sua opinião, quais são os principais riscos que podem causar erros na execução desse script?
  

